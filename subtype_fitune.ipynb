{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertConfig\n",
    "\n",
    "\n",
    "# 1. 数据加载和预处理\n",
    "class VirusDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=512):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        sequence = ''.join(c for c in sequence.upper() if c in ['A', 'T', 'G', 'C', 'N'])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def load_data(fasta_file, label_file, existing_label_to_idx=None):\n",
    "    sequences = []\n",
    "    seq_ids = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append(str(record.seq))\n",
    "        main_id = record.id.split('|')[0]\n",
    "        seq_ids.append(main_id)\n",
    "    \n",
    "    print(f\"加载了 {len(sequences)} 条序列\")\n",
    "    \n",
    "    labels_df = pd.read_csv(label_file)\n",
    "    print(f\"标签文件包含 {len(labels_df)} 条记录\")\n",
    "    id_to_label = dict(zip(labels_df['accession'], labels_df['subtype']))\n",
    "    \n",
    "    # 获取标签列表\n",
    "    labels = []\n",
    "    valid_seq_ids = []\n",
    "    valid_sequences = []\n",
    "    \n",
    "    for i, seq_id in enumerate(seq_ids):\n",
    "        if seq_id in id_to_label:\n",
    "            labels.append(id_to_label[seq_id])\n",
    "            valid_seq_ids.append(seq_id)\n",
    "            valid_sequences.append(sequences[i])\n",
    "        else:\n",
    "            print(f\"警告: 序列ID {seq_id} 在标签文件中未找到\")\n",
    "    \n",
    "    print(f\"成功匹配了 {len(labels)} 条序列的标签\")\n",
    "    \n",
    "    # 获取唯一标签并映射到数字\n",
    "    if existing_label_to_idx is None:\n",
    "        unique_labels = sorted(set(labels))\n",
    "        print(f\"共有 {len(unique_labels)} 个不同的标签类别: {unique_labels}\")\n",
    "        \n",
    "        label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "    else:\n",
    "        label_to_idx = existing_label_to_idx\n",
    "        idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "        print(f\"使用预定义的标签映射，共 {len(label_to_idx)} 个类别\")\n",
    "    \n",
    "    # 将标签转换为数字\n",
    "    numeric_labels = [label_to_idx[label] for label in labels]\n",
    "    \n",
    "    return valid_sequences, numeric_labels, label_to_idx, idx_to_label\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    # 处理predictions是元组的情况\n",
    "    if isinstance(pred.predictions, tuple):\n",
    "        # 通常第一个元素是logits\n",
    "        preds = pred.predictions[0].argmax(-1)\n",
    "    else:\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 设置参数\n",
    "    train_fasta_file = \"/root/autodl-tmp/Influenza_BERT/train_sequences_5.fasta\"\n",
    "    train_label_file = \"/root/autodl-tmp/Influenza_BERT/train_labels_5.csv\"\n",
    "    val_fasta_file = \"/root/autodl-tmp/Influenza_BERT/val_sequences_5.fasta\"\n",
    "    val_label_file = \"/root/autodl-tmp/Influenza_BERT/val_labels_5.csv\"\n",
    "    model_name = \"/root/autodl-tmp/Influenza_BERT/HNvirusBERT\"  # DNABERT2预训练模型\n",
    "    output_dir = \"test_classification_model\"\n",
    "    batch_size = 8  \n",
    "    epochs = 10\n",
    "    \n",
    "    # 加载训练数据\n",
    "    train_sequences, train_labels, label_to_idx, idx_to_label = load_data(train_fasta_file, train_label_file)\n",
    "    num_labels = len(label_to_idx)\n",
    "    \n",
    "    # 加载验证数据\n",
    "    val_sequences, val_labels, _, _ = load_data(val_fasta_file, val_label_file, label_to_idx)\n",
    "    \n",
    "    print(f\"训练集大小: {len(train_sequences)}, 验证集大小: {len(val_sequences)}\")\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)  \n",
    "    config.num_labels = num_labels  \n",
    "    config.problem_type = \"single_label_classification\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = VirusDataset(train_sequences, train_labels, tokenizer)\n",
    "    val_dataset = VirusDataset(val_sequences, val_labels, tokenizer)\n",
    "    \n",
    "    # 设置训练参数\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=2,\n",
    "        # 禁用混合精度训练\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "    )\n",
    "    \n",
    "    # 创建Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    print(\"开始训练模型...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 保存最终模型\n",
    "    trainer.save_model(output_dir)\n",
    "    \n",
    "    # 保存标签映射\n",
    "    with open(os.path.join(output_dir, \"label_mapping.txt\"), \"w\") as f:\n",
    "        for idx, label in idx_to_label.items():\n",
    "            f.write(f\"{idx}\\t{label}\\n\")\n",
    "    \n",
    "    # 评估训练集\n",
    "    print(\"\\n训练集评估结果:\")\n",
    "    train_results = trainer.evaluate(train_dataset)\n",
    "    for key, value in train_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    # 评估验证集\n",
    "    print(\"\\n验证集评估结果:\")\n",
    "    eval_results = trainer.evaluate(val_dataset)\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\n模型已保存到 {output_dir}\")\n",
    "    print(f\"标签映射已保存到 {os.path.join(output_dir, 'label_mapping.txt')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
